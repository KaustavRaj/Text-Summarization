{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summarizer Manual.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaustavRaj/Text-Summarization/blob/master/Text_Summarizer_Manual.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aqx-s_4VF3NE",
        "colab_type": "text"
      },
      "source": [
        "# Text Summarizer Manual\n",
        "##### *by Kaustav Bhattacharjee, IIIT Guwahati*\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3grVv0qiGKah",
        "colab_type": "text"
      },
      "source": [
        "Here, I'm going to show how to use the encoder-decoder model that was trained in **Text Summarization** jupyter notebook. So lets first import all the libraries and other necessities for google colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8QvQAx7chCt",
        "colab_type": "code",
        "outputId": "58263e62-0596-4610-93a2-bc561e805107",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVsDAI7Zev4U",
        "colab_type": "code",
        "outputId": "0d002f50-4af6-412a-d5de-ca45e8226dd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf8rmkwBezyl",
        "colab_type": "code",
        "outputId": "40f01f7f-b6be-44e7-8530-df7f6c371beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        }
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/85/41/c3dfd5feb91a8d587ed1a59f553f07c05f95ad4e5d00ab78702fbf8fe48a/contractions-0.0.24-py2.py3-none-any.whl\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 4.3MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 19.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81693 sha256=0678b7680ff5e1fbb82399a160664158ceeed83a8d59d196e939976d96417be4\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch, contractions\n",
            "Successfully installed Unidecode-1.1.1 contractions-0.0.24 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hq77PdHHA4f",
        "colab_type": "text"
      },
      "source": [
        "Now, to summarize a sentence, we just need the below code out of the entire 'Summarization' notebook because our trained models are already saved in google drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUYwQs-AcitJ",
        "colab_type": "code",
        "outputId": "32fb3aac-8a68-4d77-c4fd-9151de8af2c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "import numpy as np\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\n",
        "\n",
        "_MAX_TEXT_LEN    =   60\n",
        "_MAX_SUMMARY_LEN =   10\n",
        "_TEXT_PADDING    =   'post'\n",
        "\n",
        "dir_path = '/content/gdrive/My Drive/Colab Notebooks/Summarization/summarization v2'\n",
        "\n",
        "encoder_model = load_model(dir_path + '/models/encoder_model.h5')\n",
        "decoder_model = load_model(dir_path + '/models/decoder_model.h5')\n",
        "model         = load_model(dir_path + '/models/model_3.h5')\n",
        "\n",
        "\n",
        "with open(dir_path + '/data/word_indices_mapping.pickle', 'rb') as f:\n",
        "  index_to_word_text, index_to_word_summary, word_to_index_summary = pickle.load(f)\n",
        "\n",
        "\n",
        "with open(dir_path + '/data/tok_x.pickle', 'rb') as f:\n",
        "  tok_x = pickle.load(f)\n",
        "\n",
        "\n",
        "def cleaner(text, remove_stopwords=True):\n",
        "  \"\"\"removes url's, nltk's stopwords and anything which is not an alphabet\"\"\"\n",
        "  \n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text.lower(), flags=re.MULTILINE)\n",
        "  text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "  text = contractions.fix(text, slang=False)\n",
        "  if remove_stopwords:\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words]).strip()\n",
        "  return text\n",
        "\n",
        "\n",
        "def summarizer(input_seq):\n",
        "    encoder_out, encoder_h, encoder_c = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = word_to_index_summary['stok']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [encoder_out, encoder_h, encoder_c])\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = index_to_word_summary[sampled_token_index]\n",
        "        \n",
        "        if sampled_token != 'etok':\n",
        "            decoded_sentence += sampled_token + ' '\n",
        "\n",
        "        if sampled_token == 'etok' or len(decoded_sentence.split()) >= (_MAX_SUMMARY_LEN-1):\n",
        "            stop_condition = True\n",
        "\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        encoder_h, encoder_c = h, c\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "def tryit(sent):\n",
        "  \"\"\"wrapper function to test the model\"\"\"\n",
        "\n",
        "  sent = cleaner(sent, remove_stopwords=True)\n",
        "  if len(sent.split()) > _MAX_TEXT_LEN:\n",
        "    return \"make your sentence length less than {} words\".format(_MAX_TEXT_LEN)\n",
        "\n",
        "  seq = tok_x.texts_to_sequences(sent.split())\n",
        "  seq = [[item for sublist in seq for item in sublist]]\n",
        "  seq = pad_sequences(seq, maxlen=_MAX_TEXT_LEN, padding=_TEXT_PADDING)\n",
        "  return summarizer(seq.reshape(1,_MAX_TEXT_LEN))\n",
        "\n",
        "\n",
        "print(tryit('my dog loves the food'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog loves it \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SfnL_WsHwxA",
        "colab_type": "text"
      },
      "source": [
        "Let's try to see a few more sentences..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyVxS5pVci09",
        "colab_type": "code",
        "outputId": "f4e9b6e9-a117-42ae-fbc9-f6034d3dadc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tryit('actually try cups see work advertised however order arrived time excellent condition gives star rating cups work advertised rating jumps stars'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "great product but not a great price \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFPDcZcncizG",
        "colab_type": "code",
        "outputId": "4c83b509-73ba-4c20-9653-3de36faa618c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tryit('pumpkin seeds received bob redmill company stale contained lot small seed fragment')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'pumpkin seeds '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXwp3biZciqz",
        "colab_type": "code",
        "outputId": "d9a4ab2f-9784-474c-c2f0-b6cab0fe6733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tryit('guess item would find cooking however found bit bitter eating snack')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'not a good product '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMrasf4AH8ay",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbuRmqEWIBKE",
        "colab_type": "text"
      },
      "source": [
        "The above code has been made into a class, which makes it easier to work with and maybe modified into a module upon improvement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zniUaHmLiMdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import os\n",
        "import pickle\n",
        "import logging\n",
        "import numpy as np\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\n",
        "\n",
        "\n",
        "class Summarizer:\n",
        "\n",
        "  def __init__(self):\n",
        "    # change the dir_path to your folder's location if used in google colab,\n",
        "    # otherwise make it 'None' if used in own computer\n",
        "\n",
        "    self.dir_path = '/content/gdrive/My Drive/Colab Notebooks/Summarization/summarization v2'\n",
        "    self._MAX_TEXT_LEN    =  60\n",
        "    self._MAX_SUMMARY_LEN =  10\n",
        "    self._TEXT_PADDING    =  'post'\n",
        "    self.encoder_model = load_model(self.dir_path + '/models/encoder_model.h5')\n",
        "    self.decoder_model = load_model(self.dir_path + '/models/decoder_model.h5')\n",
        "    with open(self.dir_path + '/data/word_indices_mapping.pickle', 'rb') as f:\n",
        "      self.index_to_word_text, self.index_to_word_summary, self.word_to_index_summary = pickle.load(f)\n",
        "    with open(self.dir_path + '/data/tok_x.pickle', 'rb') as f:\n",
        "      self.tok_x = pickle.load(f)\n",
        "\n",
        "\n",
        "  def summarize(self, sent):\n",
        "    \"\"\"wrapper function to test the model\"\"\"\n",
        "\n",
        "    sent = self.cleaner(sent, remove_stopwords=True)\n",
        "    if len(sent.split()) > self._MAX_TEXT_LEN:\n",
        "      return \"make your sentence length less than {} words\".format(self._MAX_TEXT_LEN)\n",
        "\n",
        "    seq = self.tok_x.texts_to_sequences(sent.split())\n",
        "    seq = [[item for sublist in seq for item in sublist]]\n",
        "    seq = pad_sequences(seq, maxlen=self._MAX_TEXT_LEN, padding=self._TEXT_PADDING)\n",
        "    return self.seq2seq_model(seq.reshape(1, self._MAX_TEXT_LEN))\n",
        "\n",
        "\n",
        "  def cleaner(self, text, remove_stopwords=True):\n",
        "    \"\"\"removes url's, nltk's stopwords and anything which is not an alphabet\"\"\"\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text.lower(), flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
        "    text = contractions.fix(text, slang=False)\n",
        "    if remove_stopwords:\n",
        "      text = ' '.join([word for word in text.split() if word not in stop_words]).strip()\n",
        "      \n",
        "    return text\n",
        "  \n",
        "\n",
        "  def seq2seq_model(self, input_seq):\n",
        "    \"\"\"summarizes the input text given and returns the summarized string\"\"\"\n",
        "\n",
        "    encoder_out, encoder_h, encoder_c = self.encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0, 0] = self.word_to_index_summary['stok']\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "      output_tokens, h, c = self.decoder_model.predict([target_seq] + [encoder_out, encoder_h, encoder_c])\n",
        "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "      sampled_token = self.index_to_word_summary[sampled_token_index]\n",
        "      \n",
        "      if sampled_token != 'etok':\n",
        "        decoded_sentence += sampled_token + ' '\n",
        "        \n",
        "      if sampled_token == 'etok' or len(decoded_sentence.split()) >= (self._MAX_SUMMARY_LEN-1):\n",
        "        stop_condition = True\n",
        "\n",
        "      target_seq = np.zeros((1,1))\n",
        "      target_seq[0, 0] = sampled_token_index\n",
        "      encoder_h, encoder_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_XweWvBKdlG",
        "colab_type": "text"
      },
      "source": [
        "It's usage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rQ6OrXNkCTY",
        "colab_type": "code",
        "outputId": "b6f1d1d1-14ec-46e2-bd9a-4c27d0032f3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "our_model = Summarizer()\n",
        "our_model.summarize('after all this hardwork we should have a python party tonight')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'great for a little lifestyle '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    }
  ]
}